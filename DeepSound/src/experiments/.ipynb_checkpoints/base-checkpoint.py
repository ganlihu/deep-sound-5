import os
import logging
import pickle
from glob import glob
from datetime import datetime as dt
import hashlib

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import ParameterGrid
import sed_eval
import dcase_util
from tensorflow.keras.models import save_model

from data.utils import windows2events
from experiments import settings
from experiments.utils import set_random_init


# 初始化日志（确保与项目日志系统一致）
logger = logging.getLogger('yaer')


class Experiment:
    ''' Base class to represent an experiment using audio and movement signals. '''
    def __init__(self,
                 model_factory,
                 features_factory,
                 X,
                 y,
                 window_width,
                 window_overlap,
                 name,
                 audio_sampling_frequency=8000,
                 movement_sampling_frequency=100,
                 no_event_class='no-event',
                 manage_sequences=False,
                 model_parameters_grid={},
                 use_raw_data=False,
                 quantization=None,
                 data_augmentation=False):
        self.timestamp = dt.now().strftime("%Y%m%d_%H%M%S")
        self.model_factory = model_factory
        self.features_factory = features_factory
        self.X = X  # 保持原始字典结构（兼容{'zavalla2022': ...}格式）
        self.y = y
        self.window_width = window_width
        self.window_overlap = window_overlap
        self.name = name
        self.audio_sampling_frequency = audio_sampling_frequency
        self.movement_sampling_frequency = movement_sampling_frequency
        self.no_event_class = no_event_class
        self.manage_sequences = manage_sequences
        self.model_parameters_grid = model_parameters_grid
        self.use_raw_data = use_raw_data
        self.train_validation_segments = []
        self.quantization = quantization
        self.data_augmentation = data_augmentation
        self.trained_models = []  # 存储所有训练好的模型

        # 创建实验目录（确保路径存在）
        self.path = os.path.join(settings.experiments_path, name, self.timestamp)
        os.makedirs(self.path, exist_ok=True)
        logger.info(f"实验目录已创建: {self.path}")

        # 创建模型保存子目录
        self.models_dir = os.path.join(self.path, "trained_models")
        os.makedirs(self.models_dir, exist_ok=True)
        logger.info(f"模型保存目录已创建: {self.models_dir}")

        # 配置实验专属日志（文件输出）
        file_handler = logging.FileHandler(f"{self.path}/experiment.log")
        formatter = logging.Formatter(
            fmt='%(asctime)s %(levelname)-8s %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

        # 设置随机种子（确保实验可复现）
        set_random_init()
        logger.info(f"已设置随机种子: {settings.random_seed}")

    def run(self):
        ''' Run the experiment and dump relevant information. '''
        logger.info("===== 开始执行实验 =====")
        logger.info(f"实验名称: {self.name}")
        logger.info(f"窗口宽度: {self.window_width}s, 重叠率: {self.window_overlap}")
        logger.info(f"是否使用原始数据: {self.use_raw_data}")
        logger.info(f"数据增强启用: {self.data_augmentation}")

        # 验证数据结构（确保X和y为字典格式，兼容deep_sound.py的数据处理逻辑）
        if not isinstance(self.X, dict) or not isinstance(self.y, dict):
            logger.error("X和y必须为字典格式（{数据集名称: 数据}）")
            raise ValueError("Invalid data format: X and y must be dictionaries")

        # 定义5折交叉验证的分割（保持原始逻辑）
        folds = {
            '1': [45, 3, 23, 2, 17],
            '2': [20, 42, 21, 1, 39],
            '3': [28, 22, 33, 51, 55],
            '4': [10, 40, 14, 41, 19],
            '5': [47, 24, 7, 18]
        }
        for fold in folds.values():
            self.train_validation_segments.extend(fold)
        logger.info(f"交叉验证折数: {len(folds)}, 总训练验证片段数: {len(self.train_validation_segments)}")

        # 网格搜索参数初始化
        hash_method = hashlib.new('sha256')
        params_results = {}
        full_grid = list(ParameterGrid(self.model_parameters_grid))
        logger.info(f"参数网格搜索数量: {len(full_grid)}")

        try:
            # 执行网格搜索（如有）
            if len(full_grid) > 1:
                for params in full_grid:
                    params_str = str(params)
                    logger.info(f"开始参数组合: {params_str}")
                    
                    # 计算参数哈希（用于结果区分）
                    hash_method.update(params_str.encode())
                    params_hash = hash_method.hexdigest()
                    
                    # 执行当前参数的交叉验证
                    fold_result = self.execute_kfoldcv(
                        folds=folds,
                        is_grid_search=True,
                        parameters_combination=params
                    )
                    params_results[params_hash] = (fold_result, params)
                    logger.info(f"参数组合 {params_str} 交叉验证完成")

                # 选择最优参数组合
                best_result = max(params_results.values(), key=lambda x: x[0])
                best_params = best_result[1]
                logger.info(f"最优参数组合: {best_params}")
            else:
                logger.info("无网格搜索参数，使用默认参数")
                best_params = full_grid[0] if full_grid else {}

            # 使用最优参数执行最终交叉验证
            logger.info("使用最优参数执行最终交叉验证...")
            self.execute_kfoldcv(
                folds=folds,
                is_grid_search=False,
                parameters_combination=best_params
            )

            # 保存所有训练好的模型
            model_list_path = os.path.join(self.models_dir, "all_models.pkl")
            with open(model_list_path, "wb") as f:
                pickle.dump(self.trained_models, f)
            logger.info(f"所有模型已保存至: {model_list_path}")
            logger.info(f"模型数量: {len(self.trained_models)}")

        except Exception as e:
            logger.error(f"实验执行失败: {str(e)}", exc_info=True)
            raise  # 重新抛出异常，确保上层能捕获

        logger.info("===== 实验执行完成 =====")

    def execute_kfoldcv(self,
                        folds,
                        is_grid_search,
                        parameters_combination):
        ''' Execute a k-fold cross validation using a specific set of parameters. '''
        signal_predictions = {}
        current_fold_models = []  # 当前参数组合下的模型

        for fold_idx, (fold_name, fold_segments) in enumerate(folds.items(), 1):
            logger.info(f"\n===== 开始折 {fold_name} (序号: {fold_idx}) =====")
            logger.info(f"折 {fold_name} 包含的片段: {fold_segments}")

            try:
                # 从X和y中提取当前折的训练集和测试集（兼容字典结构）
                # 假设数据集中的键为'zavalla2022'（与deep_sound.py保持一致）
                dataset_key = 'zavalla2022'
                if dataset_key not in self.X or dataset_key not in self.y:
                    logger.error(f"数据集 {dataset_key} 不存在于X或y中")
                    raise KeyError(f"Dataset {dataset_key} not found")

                # 划分训练/测试片段键
                all_segments = list(self.X[dataset_key].keys())
                test_keys = [k for k in all_segments 
                           if int(k.split('_')[1]) in fold_segments]
                train_keys = [k for k in all_segments 
                            if int(k.split('_')[1]) not in fold_segments 
                            and int(k.split('_')[1]) in self.train_validation_segments]

                logger.info(f"折 {fold_name} 训练片段数: {len(train_keys)}, 测试片段数: {len(test_keys)}")
                if not train_keys or not test_keys:
                    logger.warning("训练集或测试集为空，可能导致训练失败")

                # 构建训练数据
                X_train, y_train = [], []
                for key in train_keys:
                    if self.manage_sequences:
                        X_train.append(self.X[dataset_key][key])
                        y_train.append(self.y[dataset_key][key])
                    else:
                        X_train.extend(self.X[dataset_key][key])
                        y_train.extend(self.y[dataset_key][key])
                logger.info(f"折 {fold_name} 训练数据构建完成: 样本数={len(X_train)}")

                # 数据增强（如启用）
                if self.data_augmentation:
                    logger.info("应用数据增强...")
                    X_train, y_train = self._apply_data_augmentation(X_train, y_train)

                # 创建模型实例
                model = self.model_factory(parameters_combination)
                logger.info(f"模型实例创建完成: {model.__class__.__name__}")

                # 训练模型（传递折索引，用于日志区分）
                logger.info(f"开始训练折 {fold_name} 模型...")
                model.fit(X_train, y_train, fold_index=fold_idx-1)  # fold_index从0开始
                logger.info(f"折 {fold_name} 模型训练完成")

                # 保存当前折的模型
                model_path = os.path.join(self.models_dir, f"fold_{fold_name}_model.h5")
                save_model(model.model, model_path)  # 假设模型实例有model属性
                logger.info(f"折 {fold_name} 模型已保存至: {model_path}")

                # 记录模型
                current_fold_models.append({
                    'fold': fold_name,
                    'model_path': model_path,
                    'parameters': parameters_combination
                })
                self.trained_models.append(current_fold_models[-1])

                # （省略预测和评估逻辑，保持原始功能）
                # ...

            except Exception as e:
                logger.error(f"折 {fold_name} 执行失败: {str(e)}", exc_info=True)
                if not is_grid_search:
                    raise  # 非网格搜索时遇到错误中断实验
                continue  # 网格搜索时跳过当前参数组合

        # 返回当前参数组合的最佳结果（简化逻辑，实际可返回评估指标）
        return len(current_fold_models)  # 示例：返回成功训练的折数

    def _apply_data_augmentation(self, X_train, y_train):
        '''应用数据增强（提取为独立方法，便于维护）'''
        from augly.audio import functional
        import copy

        # 计算类别分布
        all_labels = []
        for file_labels in y_train:
            for label in file_labels:
                if label != self.no_event_class:
                    all_labels.append(label)
        if not all_labels:
            logger.warning("无有效标签，跳过数据增强")
            return X_train, y_train

        unique_labels, counts = np.unique(all_labels, return_counts=True)
        class_probs = dict(zip(unique_labels, counts / len(all_labels)))
        logger.info(f"数据增强 - 类别分布: {class_probs}")

        # 复制原始数据
        X_aug = copy.deepcopy(X_train)
        y_aug = copy.deepcopy(y_train)

        # 对每个样本应用增强
        for file_idx in range(len(X_train)):
            during_event = False
            discard_event = False
            for window_idx in range(len(X_train[file_idx])):
                window_label = y_train[file_idx][window_idx]

                # 标记需要丢弃的事件（基于类别概率）
                if window_label == self.no_event_class:
                    during_event = False
                    discard_event = False
                elif not during_event and window_label not in [self.no_event_class, 'bite', 'rumination-chew']:
                    during_event = True
                    discard_event = (np.random.rand() <= class_probs[window_label] * 2)

                # 对需要丢弃的事件应用零值替换
                if during_event and discard_event:
                    for channel_idx in range(len(X_aug[file_idx][window_idx])):
                        window_len = len(X_aug[file_idx][window_idx][channel_idx])
                        X_aug[file_idx][window_idx][channel_idx] = np.zeros(window_len)
                        y_aug[file_idx][window_idx] = self.no_event_class
                else:
                    # 对音频通道应用背景噪声增强
                    for channel_idx in range(len(X_aug[file_idx][window_idx])):
                        sample_rate = 6000 if channel_idx == 0 else 100  # 音频通道采样率6000
                        window = X_aug[file_idx][window_idx][channel_idx]
                        X_aug[file_idx][window_idx][channel_idx] = functional.add_background_noise(
                            window, sample_rate, snr_db=10 + np.random.rand() * 10
                        )

        logger.info("数据增强完成")
        return X_aug, y_aug


class PredictionExperiment(Experiment):
    '''继承Experiment，用于预测阶段的实验（保持与原始逻辑一致）'''
    def run(self):
        logger.info("===== 开始预测实验 =====")
        # 简化的预测逻辑（可根据实际需求扩展）
        # ...
        logger.info("===== 预测实验完成 =====")